{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `skweak`: a quick demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start: preparing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a small corpus of 200 news articles that we wish to annotate with two entity types: \n",
    "- companies\n",
    "- other (non-commercial) organisations.\n",
    "\n",
    "The first step is to extract the texts from the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# We retrieve the texts\n",
    "texts = []\n",
    "archive_file = tarfile.open(\"data/reuters_small.tar.gz\")\n",
    "for archive_member in archive_file.getnames():\n",
    "    if archive_member.endswith(\".txt\"):\n",
    "        text = archive_file.extractfile(archive_member).read().decode(\"utf8\")\n",
    "        texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run Spacy on those texts to obtain `Doc` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# We run spacy on the texts    \n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "docs = list(nlp.pipe(texts))\n",
    "\n",
    "# Print an example of text:\n",
    "docs[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 1: Labelling functions\n",
    "\n",
    "Labelling functions are at the core of `skweak`. They take a `Doc` as input and returns a list of spans with their associated labels. \n",
    "\n",
    "One simple type of labelling functions are heuristics. For instance, we can write that commercial companies may be recognized by their legal suffix (such as Corp.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skweak\n",
    "\n",
    "def company_detector_fun(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk[-1].lower_.rstrip(\".\") in {'corp', 'inc', 'ltd', 'llc', 'sa', 'ag'}:\n",
    "            yield chunk.start, chunk.end, \"COMPANY\"\n",
    "\n",
    "# We create the labelling function by giving it a name, and a function to apply\n",
    "company_detector = skweak.heuristics.FunctionAnnotator(\"company_detector\", company_detector_fun)\n",
    "\n",
    "# We run the function on the full corpus\n",
    "docs = list(company_detector.pipe(docs))\n",
    "\n",
    "# Show an example\n",
    "skweak.utils.display_entities(docs[28], \"company_detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "For non-commercial organisations, we can also look for the occurrence of words that are quite typical of public organisations or NGOs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OTHER_ORG_CUE_WORDS = {\"University\", \"Institute\", \"College\", \"Committee\", \"Party\", \"Agency\",\n",
    "                       \"Union\", \"Association\", \"Organization\", \"Court\", \"Office\", \"National\"}\n",
    "def other_org_detector_fun(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any([tok.text in OTHER_ORG_CUE_WORDS for tok in chunk]):\n",
    "            yield chunk.start, chunk.end, \"OTHER_ORG\"\n",
    "\n",
    "# We create the labelling function\n",
    "other_org_detector = skweak.heuristics.FunctionAnnotator(\"other_org_detector\", other_org_detector_fun)\n",
    "\n",
    "# We run the function on the full corpus\n",
    "docs = list(other_org_detector.pipe(docs))\n",
    "\n",
    "# Show an example\n",
    "skweak.utils.display_entities(docs[28], \"other_org_detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "In addition to heuristics, we can also exploit _gazetteers_ that search for the occurrences of entries (often extracted from a knowledge base): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We extract the entries (from Crunchbase)\n",
    "tries = skweak.gazetteers.extract_json_data(\"data/crunchbase_companies.json\")\n",
    "gazetteer = skweak.gazetteers.GazetteerAnnotator(\"gazetteer\", tries)\n",
    "print(\"done building the gazetteer\")\n",
    "\n",
    "# We run the function on the full corpus\n",
    "docs = list(gazetteer.pipe(docs))\n",
    "\n",
    "# Show an example\n",
    "skweak.utils.display_entities(docs[28], \"gazetteer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "And finally, we can also take advantage of machine learning models trained from data of related domains. Here, we will use a spacy model to get the usual named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run a NER trained on conll2003\n",
    "ner = skweak.spacy.ModelAnnotator(\"spacy\", \"en_core_web_md\")\n",
    "docs = list(ner.pipe(docs))\n",
    "\n",
    "# Show an example\n",
    "skweak.utils.display_entities(docs[28], \"spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "## Step 2: aggregation\n",
    "\n",
    "Once the labelling functions have been applied, we must then aggregate their results, so that we can a single annotation for each document. This is done in `skweak` by estimating a generative model. Aggregating the labels can be done in a few lines of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the aggregation model\n",
    "model = skweak.aggregation.HMM(\"hmm\", [\"COMPANY\", \"OTHER_ORG\"])\n",
    "\n",
    "# We indicate that \"ORG\" is an underspecified value, which may\n",
    "# represent either COMPANY or OTHER_ORG\n",
    "model.add_underspecified_label(\"ORG\", [\"COMPANY\", \"OTHER_ORG\"])\n",
    "\n",
    "# And run the estimation\n",
    "docs = model.fit_and_aggregate(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skweak.utils.display_entities(docs[28], \"hmm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 3: Training the final model\n",
    "    \n",
    "Once we have finished labelling the corpus, we can then train any type of machine learning model on it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc.ents = doc.spans[\"hmm\"]\n",
    "skweak.utils.docbin_writer(docs, \"data/reuters_small.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy init config - --lang en --pipeline ner --optimize accuracy | \\\n",
    "spacy train - --paths.train ./data/reuters_small.spacy  --paths.dev ./data/reuters_small.spacy \\\n",
    "--initialize.vectors en_core_web_md --output data/reuters_small\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
